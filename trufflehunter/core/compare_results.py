import csv
from datetime import datetime
from datetime import timedelta
import json

#TODO: Needs to be replaced with the real quad1 code.
def numFilledTTLs(x_ints, max_ttl):
    # Any number of cache hits per TTL get counted as one cache hit. 
    # TTL "epochs" start at unix time 0.
    ttl_epochs = []
    for x in x_ints:
        ttl_epochs.append(int(int(x.timestamp()) / max_ttl) * max_ttl)
    return len(set(ttl_epochs))

def coalesce(x_ints):
    x_ints = sorted(set(x_ints))
    coalesced = []
    one = timedelta(seconds=1)
    two = timedelta(seconds=2)

    for i in range(2, (len(x_ints)-2)):
        lolo = x_ints[i-2]
        lo = x_ints[i-1]
        mid = x_ints[i]
        hi = x_ints[i+1]
        hihi = x_ints[i+2]

        # If I'm a group of one, count me.
        if lo < mid-one and hi > mid+one:
            coalesced.append(mid)
        # If I'm first in a group of two, don't count me.
        elif lo < mid-one and mid+one == hi and mid+two < hihi:
            continue
        # If I'm second in a group of two, append.
        elif lolo < mid-two and lo == mid-one and hi > mid+one:
            coalesced.append(mid)
        # If I'm first in a group of more than two, don't count me.
        elif lo < mid-one and hi == mid+one and hihi == mid + two:
            continue
        # If I'm part of a group that's at least 3 big, but not the first or last of the group, append.
        elif lo == mid-one and hi == mid + one:
            coalesced.append(mid)
        # If I'm the last in a group that's at least three big, don't count me.
        elif hi > mid+one and lo == mid - one:
            continue
        # Otherwise, I'm not part of a group, count me.
        else:
            coalesced.append(mid)
    return coalesced

def estimateFilledCaches(ark_data, resolver):
    x_ints = {}
    tss = {}
    ttls = {}

    for (ts, ttl, pop) in zip(ark_data['dig_ts'], ark_data['ttl'], ark_data['pop_location']):
        # if ts < datetime(2020,4,29,4,0):
        #     continue
        if pop not in x_ints:
            x_ints[pop] = [ts + timedelta(seconds=ttl)]
            tss[pop] = [ts]
            ttls[pop] = [ttl]
        else:
            x_ints[pop].append(ts + timedelta(seconds=ttl))
            tss[pop].append(ts)
            ttls[pop].append(ttl)
    
    for pop in sorted(x_ints.keys()):
        if resolver == '9.9.9.9' or resolver == '149.112.112.112' or resolver == 'OpenDNS' or '208.67' in resolver:
            # "Randall method" (remove first and last from group)
            coalesced_x_ints = coalesce(x_ints[pop])
            return (pop, len(coalesced_x_ints))
        elif resolver == '1.1.1.1' or resolver == '1.0.0.1':
            # We can only see one cache hit per TTL
            return (pop, numFilledTTLs(x_ints[pop], 10800))
        elif '8.8.' in resolver:
            ttl_lines = estimateFilledQuad8Caches(tss[pop], ttls[pop], pop)
            return (pop, len(coalesce(ttl_lines)))

# Takes data from a single Quad8 PoP
def estimateFilledQuad8Caches(ark_tss, ark_ttls, pop):
    # We must discard all TTL lines that begin at the same time as a measurement arriving,
    # because that measurement might have filled a previously-unfilled frontend cache. This
    # would have poisoned the cache by placing a domain in it.

    # Map of 
    valid_ttl_line = {}
    # List of timestamps at which a TTL line starts
    line_starts = []
    # Map of timestamps of measurements to the time at which their TTL line starts
    ts_to_line_start = {}

    for ts, ttl in zip(ark_tss, ark_ttls):
        line_start = ts - timedelta(seconds=(10799 - ttl))
        ts_to_line_start[ts] = line_start
        line_starts.append(line_start)

    # Remove duplicates and sort
    line_starts = sorted(set(line_starts))

    for l in line_starts:
        valid_ttl_line[l] = True

    # Now eliminate TTL lines generated by ark nodes' cache hits
    for ts in sorted(ark_tss):
        for line_start in line_starts:
            diff = timedelta(seconds=0)
            if ts > line_start:
                diff = ts - line_start
            else:
                diff = line_start - ts
            if diff <= timedelta(seconds=0):
                valid_ttl_line[line_start] = False
    
    # Count all valid TTL lines
    valid_lines = []
    for line_start in valid_ttl_line:
        if valid_ttl_line[line_start]:
            valid_lines.append(line_start)

    return set(valid_lines)
